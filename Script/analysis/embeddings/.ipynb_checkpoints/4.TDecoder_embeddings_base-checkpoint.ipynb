{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ped-BERT embeddings\n",
    "\n",
    "Modules: N/A <br>\n",
    "Author: Cornelia Ilin <br>\n",
    "Email: cilin@wisc.edu <br>\n",
    "Date created: June 8, 2022 <br>\n",
    "\n",
    "Citations for MLM: https://keras.io/examples/nlp/masked_language_modeling/\n",
    "\n",
    "Citations for ICD10 to ICD9: https://github.com/AtlasCUMC/ICD10-ICD9-codes-conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "#!pip install icd9cms\n",
    "from icd9cms.icd9 import search\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# user defined\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')\n",
    "import utils_dt_prep\n",
    "import utils_MLM\n",
    "import utils_MLM_eval\n",
    "import utils_embeddings\n",
    "import utils_TDecoder\n",
    "from predictions import utils_dt_prep_pred_all\n",
    "\n",
    "from sklearn.metrics import average_precision_score as APS\n",
    "from sklearn.metrics import roc_auc_score as ROC_AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "\n",
    "# opress warnings\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Set-up config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 40 #of diagnosis history\n",
    "    BATCH_SIZE = 32\n",
    "    PAT_MIN_LENGTH = 3 #minimum number of visits\n",
    "    DIAG_PER_VISIT = 3 #diagnosis per visit to consider\n",
    "    DIAG_LENGTH = 2 # how many digits from diagnosis code to consider\n",
    "    train_pct = 0.8\n",
    "    seed = 1789\n",
    "    KEYS_diag = ['diag']\n",
    "    KEYS_diag_age = ['diag', 'age']\n",
    "    KEYS_diag_cnty = ['diag', 'cnty']\n",
    "    KEYS_diag_age_cnty = ['diag', 'age', 'cnty']\n",
    "    ############################################\n",
    "    create_Xy = False\n",
    "    create_vect_layer = False\n",
    "    draw_rlnIs = False\n",
    "    create_model = True\n",
    "    run_model = True\n",
    "    plot_model = True\n",
    "    load_model = False\n",
    "    chunk_splits = True\n",
    "    chunk_split_size = 200\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``read medical records for all patients with SSN and birth records``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init_bpe = utils_dt_prep.read_data_bpe()\n",
    "\n",
    "# print shapes and head\n",
    "print('Unique patients ', df_init_bpe.rlnI_updated.nunique())\n",
    "print('Number of encounters (shape of data) ', df_init_bpe.shape)\n",
    "df_init_bpe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``read medical records for all patients with SSN (includes those with birth records)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_dt_prep)\n",
    "df_init_pe = utils_dt_prep.read_data_pe()\n",
    "\n",
    "# print shapes and head\n",
    "print('Unique patients ', df_init_pe.rlnI_updated.nunique())\n",
    "print('Number of encounters (shape of data) ', df_init_pe.shape)\n",
    "df_init_pe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``keep only non-birth records data``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from df_init_pe drop if rlnI in df_init_bpe\n",
    "rlnI_in_bpe = df_init_bpe.rlnI_updated.unique()\n",
    "df_init_pe = df_init_pe[~df_init_pe.rlnI_updated.isin(rlnI_in_bpe)]\n",
    "\n",
    "# concatenate df_init_bpe and df_init_pe\n",
    "#df_init = pd.concat([df_init_bpe, df_init_pe], axis=0)\n",
    "\n",
    "# shuffle patients\n",
    "np.random.seed(config.seed)\n",
    "shuffle = np.random.permutation(np.arange(df_init_pe.shape[0]))\n",
    "df_init = df_init_pe.iloc[shuffle, :]\n",
    "df_init.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# drop df_init_bpe and df_init_pe\n",
    "del df_init_bpe, df_init_pe\n",
    "\n",
    "# print shapes and head\n",
    "print('Unique patients ', df_init.rlnI_updated.nunique())\n",
    "print('Number of encounters (shape of data) ', df_init.shape)\n",
    "\n",
    "df_init.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many unique births if I keep only patients with at least 3 hospital/ER visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_init.copy()\n",
    "\n",
    "# keep only patients with at least 3 hospital/ER visits\n",
    "temp = temp.groupby('rlnI_updated',as_index=False).admtdate.count()\n",
    "temp = temp[temp.admtdate.ge(config.PAT_MIN_LENGTH)]\n",
    "print('Unique patients ', temp.rlnI_updated.nunique())\n",
    "temp = df_init[df_init.rlnI_updated.isin(temp.rlnI_updated.unique())]\n",
    "\n",
    "# delete temp\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``drop observations and add features``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_dt_prep)\n",
    "print('Unique patients before preprocessing', df_init.rlnI_updated.nunique())\n",
    "\n",
    "# drop obervations\n",
    "%time df = utils_dt_prep.drop_observations(df_init, config.PAT_MIN_LENGTH)\n",
    "\n",
    "# add features, includes visit summary (for diag, age, cnty)\n",
    "%time df = utils_dt_prep.add_features(df, config.DIAG_PER_VISIT, config.DIAG_LENGTH)\n",
    "\n",
    "# print stats\n",
    "print('Unique patients after preprocessing', df.rlnI_updated.nunique())\n",
    "print('Number of encounters after preprocessing (shape of data) ', df.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``create train, val, and test datasets``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.draw_rlnIs:\n",
    "    # put unique rlnI in a df\n",
    "    rlnIs = np.load(\"./data/rlnIs.npy\", allow_pickle=\"TRUE\")\n",
    "    df_rlnIs = pd.DataFrame({'rlnI_updated': rlnIs})\n",
    "\n",
    "    # split rlnIs into training and test\n",
    "    np.random.seed(config.seed)\n",
    "    train_rlnI = np.random.choice(rlnIs, int(rlnIs.shape[0]*Config.train_pct), replace=False)\n",
    "    test_rlnI = list(set(rlnIs) - set(train_rlnI))\n",
    "    \n",
    "    # save train_rlnIs and test_rlnIs\n",
    "    np.save(\"./data/train_rlnI.npy\", train_rlnI)\n",
    "    np.save(\"./data/test_rlnI.npy\", test_rlnI)\n",
    "    \n",
    "else:\n",
    "    #load rlnIs\n",
    "    train_rlnI = np.load(\"./data/train_rlnI.npy\", allow_pickle=\"TRUE\")\n",
    "    test_rlnI = np.load(\"./data/test_rlnI.npy\", allow_pickle=\"TRUE\")\n",
    "    rlnIs = np.load(\"./data/rlnIs.npy\", allow_pickle=\"TRUE\")\n",
    "    df_rlnIs = pd.DataFrame({'rlnI_updated': rlnIs})\n",
    "\n",
    "# pull train and test from df\n",
    "df_train = df[df.rlnI_updated.isin(train_rlnI)]\n",
    "df_test = df[df.rlnI_updated.isin(test_rlnI)]\n",
    "\n",
    "print('Shape of df_train ', df_train.shape)\n",
    "print('Shape of df_test', df_test.shape)\n",
    "\n",
    "print('Unique patients in df_train ', df_train.rlnI_updated.nunique())\n",
    "print('Unique patients in df_test ', df_test.rlnI_updated.nunique())\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">create input-output pairs</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_dt_prep_pred_all)\n",
    "df_train_in, df_train_out = utils_dt_prep_pred_all.input_output_pairs(df_train, config.PAT_MIN_LENGTH)\n",
    "df_test_in, df_test_out = utils_dt_prep_pred_all.input_output_pairs(df_test, config.PAT_MIN_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_dt_prep)\n",
    "\n",
    "print('df_train_in')\n",
    "print('-----------')\n",
    "%time hist_dict_train_in = utils_dt_prep.add_history(df_train_in)\n",
    "\n",
    "print('\\ndf_test_in')\n",
    "print('-----------')\n",
    "%time hist_dict_test_in = utils_dt_prep.add_history(df_test_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print example patient in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_in[df_train_in.rlnI_updated.eq('00002OYZO')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_out[df_train_out.rlnI_updated.eq('00002OYZO')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Create vocab used in MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">import data used in MLM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mlm = np.load(\"./data/X.npy\", allow_pickle=\"TRUE\").item()\n",
    "\n",
    "print('X', X_mlm.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">load vocab used in MLM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vect layer\n",
    "vect_layer = {}\n",
    "for key in config.KEYS_diag_age_cnty:\n",
    "    vect_layer[key] = tf.keras.models.load_model('./vectorizers/vect_layer_'+key)\n",
    "    vect_layer[key] = vect_layer[key].layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Create id2token and token2id mappings</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dict\n",
    "id2token = {}\n",
    "token2id = {}\n",
    "\n",
    "for key in config.KEYS_diag_age_cnty:\n",
    "    id2token[key] = dict(enumerate(vect_layer[key].get_vocabulary()))\n",
    "    token2id[key] = {y: x for x, y in id2token[key].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 6: Encode data based on MLM vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Create features and labels</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dict\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "\n",
    "# add key and value for each feature\n",
    "for key in config.KEYS_diag_age_cnty:\n",
    "    X_train[key] = hist_dict_train_in[key]['pat_'+key+'_hist'].values\n",
    "    X_test[key] = hist_dict_test_in[key]['pat_'+key+'_hist'].values\n",
    "\n",
    "# create outcome for each patient\n",
    "df_train_out['diag00_2d'] = df_train_out['diag00_2d'].astype(str)\n",
    "df_test_out['diag00_2d'] = df_test_out['diag00_2d'].astype(str)\n",
    "y_train = df_train_out.diag00_2d # predict main diagnosis code in next visit\n",
    "y_test = df_test_out.diag00_2d # predict main diagnosis code in next visit\n",
    "\n",
    "print('Shape of X_train_diag ', X_train['diag'].shape)\n",
    "print('Shape of X_train_age ', X_train['age'].shape)\n",
    "print('Shape of y_train ', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Encode features</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dict\n",
    "X_train_tokenized = {}\n",
    "X_test_tokenized = {}\n",
    "\n",
    "for key in config.KEYS_diag_age_cnty:\n",
    "    X_train_tokenized[key] = utils_dt_prep.encode(vect_layer[key], X_train[key])\n",
    "    X_test_tokenized[key] = utils_dt_prep.encode(vect_layer[key], X_test[key])\n",
    "\n",
    "# print shape\n",
    "print('Shape of X_train_tokenized ', X_train_tokenized['diag'].shape)\n",
    "print('Shape of X_test_tokenized ', X_test_tokenized['age'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Encode outcomes</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union of train and test\n",
    "y_union = np.union1d(y_train, y_test).tolist()\n",
    "y_union_tokenized = utils_dt_prep.encode(vect_layer['diag'], y_union)[:,0]\n",
    "y_union_tokenized = np.unique(y_union_tokenized, axis=0) #token 1 shows up two times\n",
    "\n",
    "# train\n",
    "y_train_tokenized = utils_dt_prep.encode(vect_layer['diag'], y_train)[:,0]\n",
    "y_train_tokenized = pd.get_dummies(y_train_tokenized, drop_first=False).reindex(columns = y_union_tokenized, fill_value=0)\n",
    "y_train_tokenized_cols = np.array(y_train_tokenized.columns)\n",
    "y_train_tokenized = y_train_tokenized.to_numpy()\n",
    "\n",
    "# test\n",
    "y_test_tokenized = utils_dt_prep.encode(vect_layer['diag'], y_test)[:,0]\n",
    "y_test_tokenized = pd.get_dummies(y_test_tokenized, drop_first=False).reindex(columns = y_union_tokenized, fill_value=0)\n",
    "y_test_tokenized_cols = np.array(y_test_tokenized.columns)\n",
    "y_test_tokenized = y_test_tokenized.to_numpy()\n",
    "\n",
    "print('Shape y_train_tokenized ', y_train_tokenized.shape)\n",
    "print('Shape y_test_tokenized ', y_test_tokenized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect non-encoded and encoded histories and outcome in next visit for the first patient in my training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient history\n",
    "for key in config.KEYS_diag_age_cnty:\n",
    "    print(key, 'nenc: ', X_train[key][0])\n",
    "    print(key, 'enc: ', X_train_tokenized[key][0])\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient outcome in the next visit (first 20)\n",
    "y_train_tokenized[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tokenized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">!! Decide what embeddings you will use</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``for train``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = config.KEYS_diag\n",
    "\n",
    "# define subset of X_train_tokenized\n",
    "X_train_tokenized_subset = {}\n",
    "for key in keys:\n",
    "    X_train_tokenized_subset[key] = X_train_tokenized[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``for test``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = config.KEYS_diag\n",
    "\n",
    "# define subset of X_train_tokenized\n",
    "X_test_tokenized_subset = {}\n",
    "for key in keys:\n",
    "    X_test_tokenized_subset[key] = X_test_tokenized[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Shape of train data ', X_train_tokenized_subset['diag'].shape)\n",
    "print('Shape of test data ', X_test_tokenized_subset['diag'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tokenized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tokenized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Convert train, val, and test subsets to tensors</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(config.seed)\n",
    "\n",
    "# create sample_weights (this is how the MLM was trained)\n",
    "sample_weights_train = np.ones(y_train_tokenized.shape[0])\n",
    "\n",
    "train_tensor = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train_tokenized_subset, y_train_tokenized, sample_weights_train)))\n",
    "train_tensor = train_tensor.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(config.seed)\n",
    "\n",
    "# create sample_weights (this is how the MLM was trained)\n",
    "sample_weights_test = np.ones(y_test_tokenized.shape[0])\n",
    "\n",
    "test_tensor = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_test_tokenized_subset, y_test_tokenized, sample_weights_test)))\n",
    "test_tensor = test_tensor.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Step 5: Pre-training TDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Build model``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(utils_TDecoder)\n",
    "if config.create_model:\n",
    "    TDecoder_model = utils_TDecoder.create_TDecoder(vect_layer, sample_weights_train, keys, y_train_tokenized.shape[1])\n",
    "    TDecoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Fit and save model``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.run_model:\n",
    "    # fit model\n",
    "    history = TDecoder_model.fit(\n",
    "        train_tensor,\n",
    "        epochs=15,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # save model\n",
    "    if len(keys)==1:\n",
    "        model_name='_base'\n",
    "    if len(keys)==2 and keys[1]=='age':\n",
    "        model_name='_age'\n",
    "    if len(keys)==2 and keys[1]=='cnty':\n",
    "        model_name='_cnty'\n",
    "    if len(keys)==3:\n",
    "        model_name='_age_cnty'\n",
    "\n",
    "    TDecoder_model.save(\"TDecoder\" + model_name +\".h5\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.plot_model:\n",
    "    hist = history.history\n",
    "    x_arr = np.arange(len(hist['loss'])) + 1\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(x_arr, hist['loss'], '-o', label='Train loss')\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_xlabel('Epoch', size=15)\n",
    "    ax.set_ylabel('Loss', size=15)\n",
    "\n",
    "    #ax.set_ylim(0, 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Step 6: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you start code from here.... import the model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained bert model\n",
    "if config.load_model:\n",
    "    TDecoder_model = keras.models.load_model(\n",
    "        \"./TDecoder_base.h5\")\n",
    "    \n",
    "    # set model name\n",
    "    if len(keys)==1:\n",
    "        model_name='_base'\n",
    "    if len(keys)==2 and keys[1]=='age':\n",
    "        model_name='_age'\n",
    "    if len(keys)==2 and keys[1]=='cnty':\n",
    "        model_name='_cnty'\n",
    "    if len(keys)==3:\n",
    "        model_name='_age_cnty'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Evaluation (on training and test data)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_tokenized_pred = classifier_model.predict(X_train_tokenized_subset)\n",
    "#y_train_tokenized_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_tokenized_pred = classifier_model.predict(X_test_tokenized_subset)\n",
    "#y_test_tokenized_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Evaluation (if large data, chunk data)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed if the data is to large: I split the training and test data into XXX chunks and take the average value of APS, and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.chunk_splits:\n",
    "    # define global dictionaries\n",
    "    aps_master={'train':[], 'test':[]}\n",
    "    auc_master={'train':[], 'test':[]}\n",
    "\n",
    "    # define empty dictionaries\n",
    "    y_sample = {}\n",
    "    y_sample_pred = {}\n",
    "    \n",
    "    # define indeces\n",
    "    train_idx = np.arange(y_train_tokenized.shape[0])\n",
    "    test_idx = np.arange(y_test_tokenized.shape[0])\n",
    "    \n",
    "    # create dictionary with train and test data\n",
    "    X_tokenized_subset = {\n",
    "        'train': X_train_tokenized_subset,\n",
    "        'test': X_train_tokenized_subset,\n",
    "    }\n",
    "    \n",
    "    y = {\n",
    "        'train': y_train_tokenized,\n",
    "        'test': y_test_tokenized\n",
    "    }\n",
    "    \n",
    "    # add indeces to dictionary and split them into XXX chunks\n",
    "    dt_idx = {}\n",
    "    dt_idx['train'] =  np.array_split(train_idx, config.chunk_split_size)[0:2]\n",
    "    dt_idx['test'] =  np.array_split(test_idx, config.chunk_split_size)[0:2]\n",
    "\n",
    "    for dt in ['train', 'test']:\n",
    "        print('-----')\n",
    "        print(dt)\n",
    "        print('-----')\n",
    "        for idx, part in enumerate(dt_idx[dt]):\n",
    "            if idx%20==0:\n",
    "                print('Partitions executed: ', idx, '/', len(dt_idx[dt]))\n",
    "            # define example (X)\n",
    "            X_sample = {}\n",
    "            for key in X_tokenized_subset[dt].keys():\n",
    "                X_sample[key] = X_tokenized_subset[dt][key][part, :]\n",
    "            #display('Sample tokens ', sample_tokens)\n",
    "\n",
    "            # retrieve ground truth\n",
    "            y_sample[dt] = y[dt][part, :]\n",
    "\n",
    "            # predict\n",
    "            y_sample_pred[dt] = TDecoder_model.predict(X_sample)\n",
    "\n",
    "            # print shape\n",
    "            #print('Shape of TDecoder_sample_prediction ', y_sample_pred[dt].shape)\n",
    "\n",
    "            # compute APS and AUC\n",
    "            temp_metrics = {}\n",
    "            \n",
    "            aps_samples = []\n",
    "            fpr_micro= []\n",
    "            tpr_micro = []\n",
    "            area_micro = []\n",
    "\n",
    "            # compute average precision score\n",
    "            aps_samples = APS(\n",
    "                y_sample[dt],\n",
    "                y_sample_pred[dt],\n",
    "                average='samples'\n",
    "            )\n",
    "            print('APS:', aps_samples)\n",
    "\n",
    "            # ROC curve and ROC area\n",
    "            fpr_micro, tpr_micro, _ = roc_curve(\n",
    "                y_sample[dt].ravel(),\n",
    "                y_sample_pred[dt].ravel()\n",
    "            )\n",
    "            area_micro = auc(fpr_micro, tpr_micro)\n",
    "            print('AUC:', area_micro)\n",
    "\n",
    "\n",
    "            # add ROC area and APS to a df\n",
    "            temp_metrics[dt] = pd.DataFrame(\n",
    "                {'metric': ['APS', 'AUC'],\n",
    "                 'value': [aps_samples, area_micro]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # append to global list\n",
    "            aps_master[dt].append(temp_metrics[dt][temp_metrics[dt].metric.eq('APS')].value)\n",
    "            auc_master[dt].append(temp_metrics[dt][temp_metrics[dt].metric.eq('AUC')].value)\n",
    "                  \n",
    "            \n",
    "    # print average metrics    \n",
    "    for dt in ['train', 'test']:\n",
    "        print(dt + ' APS: %.3f'% (np.mean(aps_master[dt])))\n",
    "    print('----')\n",
    "    \n",
    "    for dt in ['train', 'test']:\n",
    "        print(dt + ' AUC: %.3f'% (np.mean(auc_master[dt])))\n",
    "    print('----')\n",
    "    \n",
    "    \n",
    "    # export amerage metrics to csv\n",
    "    metrics = pd.DataFrame()\n",
    "    for dt in ['train', 'test']:\n",
    "        temp= pd.DataFrame({\n",
    "            'metric':['APS', 'AUC'],\n",
    "            'value':[np.mean(aps_master[dt]),np.mean(auc_master[dt])],\n",
    "            'sample':[dt, dt]\n",
    "            })\n",
    "        metrics = pd.concat([metrics, temp], axis=0)\n",
    "    \n",
    "    metrics.reset_index(drop=True, inplace=True)\n",
    "    metrics.to_csv('../embeddings/results/TDecoder_ApsAuc_' + model_name + '.csv')\n",
    "    \n",
    "# prin number of examples in train and test\n",
    "print('Train examples:', len(train_idx))\n",
    "print('Test examples:', len(test_idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
