{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ped-BERT embeddings\n",
    "\n",
    "Modules: N/A <br>\n",
    "Author: Cornelia Ilin <br>\n",
    "Email: cilin@wisc.edu <br>\n",
    "Date created: June 8, 2022 <br>\n",
    "\n",
    "Citations for MLM: https://keras.io/examples/nlp/masked_language_modeling/\n",
    "\n",
    "Citations for ICD10 to ICD9: https://github.com/AtlasCUMC/ICD10-ICD9-codes-conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "#!pip install icd9cms\n",
    "from icd9cms.icd9 import search\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# user defined\n",
    "import utils_dt_prep\n",
    "import utils_MLM\n",
    "import utils_MLM_eval\n",
    "import utils_embeddings\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "\n",
    "# opress warnings\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Set-up config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 40 #of diagnosis history\n",
    "    BATCH_SIZE = 32\n",
    "    PAT_MIN_LENGTH = 3 #minimum number of visits\n",
    "    DIAG_PER_VISIT = 3 #diagnosis per visit to consider\n",
    "    DIAG_LENGTH = 2 # how many digits from diagnosis code to consider\n",
    "    train_pct = 0.8\n",
    "    seed = 1789\n",
    "    KEYS_diag = ['diag']\n",
    "    KEYS_diag_age = ['diag', 'age']\n",
    "    KEYS_diag_cnty = ['diag', 'cnty']\n",
    "    KEYS_diag_age_cnty = ['diag', 'age', 'cnty']\n",
    "    ############################################\n",
    "    create_Xy = False\n",
    "    create_vect_layer = False\n",
    "    draw_rlnIs = False\n",
    "    create_model = True\n",
    "    run_model = True\n",
    "    plot_model = True\n",
    "    load_model = False\n",
    "    chunk_splits = True\n",
    "    chunk_split_size = 2\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``read medical records for all patients with SSN and birth records``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init_bpe = utils_dt_prep.read_data_bpe()\n",
    "\n",
    "# print shapes and head\n",
    "print('Unique patients ', df_init_bpe.rlnI_updated.nunique())\n",
    "print('Number of encounters (shape of data) ', df_init_bpe.shape)\n",
    "df_init_bpe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``read medical records for all patients with SSN (includes those with birth records)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_dt_prep)\n",
    "df_init_pe = utils_dt_prep.read_data_pe()\n",
    "\n",
    "# print shapes and head\n",
    "print('Unique patients ', df_init_pe.rlnI_updated.nunique())\n",
    "print('Number of encounters (shape of data) ', df_init_pe.shape)\n",
    "df_init_pe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init_pe.groupby(['data_source'],as_index=False).bthyearI.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``keep only non-birth records data``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from df_init_pe drop if rlnI in df_init_bpe\n",
    "rlnI_in_bpe = df_init_bpe.rlnI_updated.unique()\n",
    "df_init_pe = df_init_pe[~df_init_pe.rlnI_updated.isin(rlnI_in_bpe)]\n",
    "\n",
    "# concatenate df_init_bpe and df_init_pe\n",
    "#df_init = pd.concat([df_init_bpe, df_init_pe], axis=0)\n",
    "\n",
    "# shuffle patients\n",
    "np.random.seed(config.seed)\n",
    "shuffle = np.random.permutation(np.arange(df_init_pe.shape[0]))\n",
    "df_init = df_init_pe.iloc[shuffle, :]\n",
    "df_init.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# drop df_init_bpe and df_init_pe\n",
    "del df_init_bpe, df_init_pe\n",
    "\n",
    "# print shapes and head\n",
    "print('Unique patients ', df_init.rlnI_updated.nunique())\n",
    "print('Number of encounters (shape of data) ', df_init.shape)\n",
    "\n",
    "df_init.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many unique births if I keep only patients with at least 3 hospital/ER visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_init.copy()\n",
    "\n",
    "# keep only patients with at least 3 hospital/ER visits\n",
    "temp = temp.groupby('rlnI_updated',as_index=False).admtdate.count()\n",
    "temp = temp[temp.admtdate.ge(config.PAT_MIN_LENGTH)]\n",
    "print('Unique patients ', temp.rlnI_updated.nunique())\n",
    "temp = df_init[df_init.rlnI_updated.isin(temp.rlnI_updated.unique())]\n",
    "\n",
    "# delete temp\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``drop observations and add features``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_dt_prep)\n",
    "print('Unique patients before preprocessing', df_init.rlnI_updated.nunique())\n",
    "\n",
    "# drop pbervations\n",
    "%time df = utils_dt_prep.drop_observations(df_init, config.PAT_MIN_LENGTH)\n",
    "\n",
    "# add features, includes visit summary (for diag, age, cnty)\n",
    "%time df = utils_dt_prep.add_features(df, config.DIAG_PER_VISIT, config.DIAG_LENGTH)\n",
    "\n",
    "# print stats\n",
    "print('Unique patients after preprocessing', df.rlnI_updated.nunique())\n",
    "print('Number of encounters after preprocessing (shape of data) ', df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add patient history summary (diag, age, cnty)\n",
    "importlib.reload(utils_dt_prep)\n",
    "\n",
    "# add patient history summary (for diag, age, cnty); returns dictionary\n",
    "%time hist_dict = utils_dt_prep.add_history(df)\n",
    "# keep track of rlnIs\n",
    "rlnIs = np.array(hist_dict['diag'].rlnI_updated)\n",
    "\n",
    "# print shape of data\n",
    "print('Unique patients after history preprocessing ', hist_dict['diag'].rlnI_updated.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to df, numpy or csv\n",
    "df.to_csv(\"./data/df.csv\")\n",
    "np.save(\"./data/hist_dict.npy\", hist_dict)\n",
    "np.save(\"./data/rlnIs.npy\", hist_dict['diag'].rlnI_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the diagnosis, age, seg, pos history for the first patient in my data, to make sure the data preprocessing step worked as planned. \n",
    "\n",
    "Note that the beginning of a patient medical history is marked with ('[CLS]') and hospital visits are separated by ('[SEP]'). For each patient visit, I only include the top 5 diagnosis codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in config.KEYS_diag_age_cnty:\n",
    "    print(key, ': ', hist_dict[key]['pat_'+key+'_hist'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``define features and outcome for MLM``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features represent the diagnosis, age, seg, and pos history of each patient. \n",
    "\n",
    "Outcome is a masked diganosis code (this is what I am trying to predict with the Bert MLM model; I set everything to -1 at first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.create_Xy:\n",
    "    # create empty dict\n",
    "    X = {}\n",
    "\n",
    "    # add key and value for each feature\n",
    "    for key in config.KEYS_diag_age_cnty:\n",
    "        X[key] = hist_dict[key]['pat_'+key+'_hist'].values\n",
    "\n",
    "    # create outcome for each patient\n",
    "    y = -1 * np.ones(X['diag'].shape, dtype=int)  # set everything to -1\n",
    "\n",
    "    print('Shape of X_diag ', X['diag'].shape)\n",
    "    print('Shape of y ', y.shape)\n",
    "\n",
    "    # save\n",
    "    np.save('./data/X.npy', X)\n",
    "    np.save('./data/y.npy', y)\n",
    "    \n",
    "else:\n",
    "    # load data\n",
    "    X = np.load(\"./data/X.npy\", allow_pickle=\"TRUE\").item()\n",
    "    y = np.load(\"./data/y.npy\", allow_pickle=\"TRUE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Get vectorize layers</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step extracts the unique diagnosis, age, and cnty codes in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config.create_vect_layer:\n",
    "    # create vectorize layers (vl) and extract vocabulary\n",
    "    importlib.reload(utils_dt_prep)\n",
    "\n",
    "    start_time= time.time()\n",
    "    # create empty dict\n",
    "    vect_layer = {}\n",
    "    for key in config.KEYS_diag_age_cnty:\n",
    "        print('Key: ', key)\n",
    "        if key=='diag':\n",
    "            vect_layer[key] = utils_dt_prep.get_vectorize_layer(\n",
    "                X[key],\n",
    "                config.MAX_LEN,\n",
    "                special_tokens=[\"[MASK]\"])\n",
    "        else:\n",
    "            vect_layer[key] = utils_dt_prep.get_vectorize_layer(\n",
    "                X[key],\n",
    "                config.MAX_LEN)\n",
    "\n",
    "    # print execution time\n",
    "    print('Execution time:', np.round((time.time()-start_time)/60, 2), 'minutes')\n",
    "    \n",
    "    # save vectorize layers\n",
    "    for key in config.KEYS_diag_age_cnty:\n",
    "        utils_dt_prep.save_vectorize_layer(vect_layer, key)\n",
    "\n",
    "else:\n",
    "    # load vect layer\n",
    "    vect_layer = {}\n",
    "    for key in config.KEYS_diag_age_cnty:\n",
    "        vect_layer[key] = tf.keras.models.load_model('./vectorizers/vect_layer_'+key)\n",
    "        vect_layer[key] = vect_layer[key].layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mask token id for masked language model\n",
    "mask_token_id = vect_layer['diag']([\"[MASK]\"]).numpy()[0][0]\n",
    "print('ID of masked token', mask_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My vocabulary contains the **padding** token ('') and OOV token ('[UNK]') as well as the passed tokens ('[CLS]', '[SEP]', and '[MASK]' if key=='diag').\n",
    "\n",
    "Below I inspect the first 10 tokens in my vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in config.KEYS_diag_age_cnty:\n",
    "    print(key, ': ', vect_layer[key].get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Encode</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to map each unique diagnosis, age, seg, pos (token) in my vocabulary to a unique integer. The TextVectorization class provides an Encoder, which I will use to create a mapping between tokens and corresponding integers.\n",
    "\n",
    "Max sequence length is set to config.MAX_LEN. If a diagnosis, age, seg, pos history is less than MAX_LEN then padding is performed by adding 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_dt_prep)\n",
    "\n",
    "# create empty dict\n",
    "X_tokenized = {}\n",
    "\n",
    "for key in config.KEYS_diag_age_cnty:\n",
    "    %time X_tokenized[key] = utils_dt_prep.encode(vect_layer[key], X[key])\n",
    "\n",
    "# print shape\n",
    "print('Shape of X_diag_tokenized ', X_tokenized['diag'].shape)\n",
    "print('Shape of X_age_tokenized ', X_tokenized['age'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect non-encoded and encoded histories for the first patient in my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in config.KEYS_diag_age_cnty:\n",
    "    print(key, 'nenc: ', X[key][0])\n",
    "    print(key, 'enc: ', X_tokenized[key][0])\n",
    "    print('-----------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Create id2token and token2id mappings</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dict\n",
    "id2token = {}\n",
    "token2id = {}\n",
    "\n",
    "for key in config.KEYS_diag_age_cnty:\n",
    "    id2token[key] = dict(enumerate(vect_layer[key].get_vocabulary()))\n",
    "    token2id[key] = {y: x for x, y in id2token[key].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Get masked input and labels. Transform to Batched Tensors</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_dt_prep)\n",
    "\n",
    "# find encoded val of CLS\n",
    "loc_CLS = token2id['diag']['[CLS]']\n",
    "\n",
    "# create MLM data\n",
    "X_diag_masked, y_masked, sample_weights = utils_dt_prep.get_masked_input_and_labels(\n",
    "    X_tokenized['diag'], mask_token_id, loc_CLS\n",
    ")\n",
    "\n",
    "# replace X_tokenized_diag with X_diag_masked\n",
    "X_tokenized['diag'] = X_diag_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_masked[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_diag_masked[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">!! Decide what embeddings you will use</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = config.KEYS_diag_cnty # other options are config.KEYS_diag_seg_pos_age_zip, config.KEYS_diag_seg_pos\n",
    "\n",
    "# define subset of X_tokenized\n",
    "X_tokenized_subset = {}\n",
    "for key in keys:\n",
    "    X_tokenized_subset[key] = X_tokenized[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Split data into training and test</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.draw_rlnIs:\n",
    "    # put unique rlnI in a df\n",
    "    rlnIs = np.load(\"./data/rlnIs.npy\", allow_pickle=\"TRUE\")\n",
    "    df_rlnIs = pd.DataFrame({'rlnI_updated': rlnIs})\n",
    "\n",
    "    # split rlnIs into training and test\n",
    "    np.random.seed(config.seed)\n",
    "    train_rlnI = np.random.choice(rlnIs, int(rlnIs.shape[0]*Config.train_pct), replace=False)\n",
    "    test_rlnI = list(set(rlnIs) - set(train_rlnI))\n",
    "    \n",
    "    # save train_rlnIs and test_rlnIs\n",
    "    np.save(\"./data/train_rlnI.npy\", train_rlnI)\n",
    "    np.save(\"./data/test_rlnI.npy\", test_rlnI)\n",
    "    \n",
    "else:\n",
    "    #load rlnIs\n",
    "    train_rlnI = np.load(\"./data/train_rlnI.npy\", allow_pickle=\"TRUE\")\n",
    "    test_rlnI = np.load(\"./data/test_rlnI.npy\", allow_pickle=\"TRUE\")\n",
    "    df_rlnIs = pd.DataFrame({'rlnI_updated': rlnIs})\n",
    "\n",
    "# grab indexes\n",
    "train_idx = list(df_rlnIs[df_rlnIs.rlnI_updated.isin(train_rlnI)].index)\n",
    "test_idx = list(df_rlnIs[df_rlnIs.rlnI_updated.isin(test_rlnI)].index)\n",
    "\n",
    "# grab train data \n",
    "X_tokenized_subset_train = {}\n",
    "for key in X_tokenized_subset.keys():\n",
    "    X_tokenized_subset_train[key] = X_tokenized_subset[key][train_idx, :]\n",
    "    \n",
    "y_masked_train = y_masked[train_idx, :]\n",
    "sample_weights_train = sample_weights[train_idx, :]\n",
    "\n",
    "# grab test data \n",
    "X_tokenized_subset_test = {}\n",
    "for key in X_tokenized_subset.keys():\n",
    "    X_tokenized_subset_test[key] = X_tokenized_subset[key][test_idx, :]\n",
    "    \n",
    "y_masked_test = y_masked[test_idx, :]\n",
    "sample_weights_test = sample_weights[test_idx, :]\n",
    "\n",
    "print('Shape of train data ', X_tokenized_subset_train['diag'].shape)\n",
    "print('Shape of test data ', X_tokenized_subset_test['diag'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Tranform to tensor</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "mlm_tensor_train = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_tokenized_subset_train, y_masked_train, sample_weights_train)\n",
    ")\n",
    "mlm_tensor_train = mlm_tensor_train.shuffle(1000).batch(config.BATCH_SIZE)\n",
    "\n",
    "# test data\n",
    "mlm_tensor_test = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_tokenized_subset_test, y_masked_test, sample_weights_test)\n",
    ")\n",
    "mlm_tensor_test = mlm_tensor_test.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Step 5: Pre-training BERT for MLM\n",
    "\n",
    "I will create a BERT-like pretraining model architecture\n",
    "using the `MultiHeadAttention` layer.\n",
    "\n",
    "It will take token ids as inputs (including masked tokens)\n",
    "and it will predict the correct ids for the masked input tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Create a medical history example to monitor MLM predictions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_MLM)\n",
    "\n",
    "# define example\n",
    "sample_tokens = {}\n",
    "for key in X_tokenized_subset_train.keys():\n",
    "    sample_tokens[key] = X_tokenized_subset_train[key][2:3,:]\n",
    "display('Sample tokens ', sample_tokens)\n",
    "\n",
    "# define monitor\n",
    "generator_callback = utils_MLM.MaskedTextGenerator(vect_layer,\n",
    "    sample_tokens, id2token, token2id, mask_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Build model``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config.create_model:\n",
    "    bert_masked_model = utils_MLM.create_masked_language_bert_model(vect_layer, sample_weights_train, keys)\n",
    "    bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Fit and save model``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.run_model:\n",
    "    # fit model\n",
    "    history = bert_masked_model.fit(\n",
    "        mlm_tensor_train,\n",
    "        epochs=15,\n",
    "        verbose=1,\n",
    "        #callbacks=[generator_callback]\n",
    "    )\n",
    "\n",
    "    # save model\n",
    "    if len(keys)==1:\n",
    "        model_name='_base'\n",
    "    if len(keys)==2 and keys[1]=='age':\n",
    "        model_name='_age'\n",
    "    if len(keys)==2 and keys[1]=='cnty':\n",
    "        model_name='_cnty'\n",
    "    if len(keys)==3:\n",
    "        model_name='_age_cnty'\n",
    "\n",
    "    bert_masked_model.save(\"bert_mlm\" + model_name +\".h5\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.plot_model:\n",
    "    hist = history.history\n",
    "    x_arr = np.arange(len(hist['loss'])) + 1\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(x_arr, hist['loss'], '-o', label='Train loss')\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_xlabel('Epoch', size=15)\n",
    "    ax.set_ylabel('Loss', size=15)\n",
    "\n",
    "    #ax.set_ylim(0, 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Step 6: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you start code from here.... import the model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained bert model\n",
    "if config.load_model:\n",
    "    bert_masked_model = keras.models.load_model(\n",
    "        \"bert_mlm_cnty.h5\", custom_objects={\"MaskedLanguageModel\": utils_MLM.MaskedLanguageModel})\n",
    "    \n",
    "    # set model name\n",
    "    if len(keys)==1:\n",
    "        model_name='_base'\n",
    "    if len(keys)==2 and keys[1]=='age':\n",
    "        model_name='_age'\n",
    "    if len(keys)==2 and keys[1]=='cnty':\n",
    "        model_name='_cnty'\n",
    "    if len(keys)==3:\n",
    "        model_name='_age_cnty'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Create top K diagnosis predictions for an example patient in X_tokenized_subset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define example (X)\n",
    "sample_tokens = {}\n",
    "for key in X_tokenized_subset.keys():\n",
    "    sample_tokens[key] = X_tokenized_subset[key][test_idx, :][2:3,:]\n",
    "#display('Sample tokens ', sample_tokens)\n",
    "\n",
    "# retrieve ground truth\n",
    "y_sample = y_masked[test_idx, :][2:3, :]\n",
    "\n",
    "# predict\n",
    "mlm_sample_prediction = bert_masked_model.predict(sample_tokens)\n",
    "\n",
    "# print shape\n",
    "print('Shape of mlm_tensor_predictions ', mlm_sample_prediction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension represents the number of examples (unique patients), the second dimension the length of the medical history for each patient (MAX_LEN), and the third dimension represents the diagnosis vocabulary size (probabilities for each word in the vocab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print predictions\n",
    "#importlib.reload(utils_MLM_eval)\n",
    "#utils_MLM_eval.one_patient_K_predictions(\n",
    "#    sample_tokens, y_sample[train_idx, :],\n",
    "#    mlm_sample_prediction, id2token['diag'],\n",
    "#    mask_token_id\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Evaluation (if small data)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_MLM_eval)\n",
    "if not config.chunk_splits:\n",
    "    ## Create top K diagnosis predictions for all patients in X_tokenized_subset. Report Accuracy ##\n",
    "    ################################################################################################\n",
    "    # define empty dictionaries\n",
    "    mlm_sample_prediction = {}\n",
    "    sample_sample_weights = {}\n",
    "    y_sample = {}\n",
    "\n",
    "    # add indeces to dictionary\n",
    "    dt_idx = {}\n",
    "    dt_idx['train'] = train_idx\n",
    "    dt_idx['test'] = test_idx\n",
    "\n",
    "    for dt in ['train', 'test']:\n",
    "        # define example (X)\n",
    "        sample_tokens = {}\n",
    "        for key in X_tokenized_subset.keys():\n",
    "            sample_tokens[key] = X_tokenized_subset[key][dt_idx[dt], :]\n",
    "        #display('Sample tokens ', sample_tokens)\n",
    "\n",
    "        # retrieve ground truth\n",
    "        y_sample[dt] = y_masked[dt_idx[dt], :]\n",
    "\n",
    "        # pull masked token indexes\n",
    "        sample_sample_weights[dt] = sample_weights[dt_idx[dt], :]\n",
    "\n",
    "        # predict\n",
    "        mlm_sample_prediction[dt] = bert_masked_model.predict(sample_tokens)\n",
    "\n",
    "        # print shape\n",
    "        print('Shape of mlm_tensor_predictions ', mlm_sample_prediction[dt].shape)\n",
    "        \n",
    "        \n",
    "    # print predictions\n",
    "    for dt in ['train', 'test']:\n",
    "        accuracy = utils_MLM_eval.all_patients_K_predictions_and_accuracy(\n",
    "            sample_tokens, y_sample[dt],\n",
    "            mlm_sample_prediction[dt], id2token['diag'],\n",
    "            mask_token_id\n",
    "        )\n",
    "\n",
    "        print(dt + ' accuracy: %.3f'% (accuracy))\n",
    "             \n",
    "    ## Create diagnosis predictions for all patients in X_tokenized_subset. Report APS and AUC ##\n",
    "    ##############################################################################################\n",
    "    # print predictions\n",
    "    metrics = {}\n",
    "    for dt in ['train', 'test']:\n",
    "        print(dt)\n",
    "        print('------')\n",
    "        metrics[dt] = utils_MLM_eval.all_patients_predictions_and_apc(\n",
    "            vect_layer,\n",
    "            sample_tokens, y_sample[dt],\n",
    "            mlm_sample_prediction[dt], id2token['diag'],\n",
    "            sample_sample_weights[dt])\n",
    "\n",
    "        # add sample name\n",
    "        metrics[dt]['sample'] = dt\n",
    "        \n",
    "    # export metrics to csv\n",
    "    metrics = pd.concat([metrics['train'], metrics['test']], axis=0)\n",
    "    metrics.to_csv('../embeddings/results/ApsAuc_' + model_name + '.csv')\n",
    "    \n",
    "    # prin number of examples in train and test\n",
    "print('Train examples:', len(train_idx))\n",
    "print('Test examples:', len(test_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:chocolate\">Evaluation (if large data, chunk data)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed if the data is to large: I split the training and test data into XXX chunks and take the average value of ACC, APS, and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.chunk_splits:\n",
    "    # define global dictionaries\n",
    "    acc_master={'train':[], 'test':[]}\n",
    "    aps_master={'train':[], 'test':[]}\n",
    "    auc_master={'train':[], 'test':[]}\n",
    "\n",
    "    # define empty dictionaries\n",
    "    mlm_sample_prediction = {}\n",
    "    sample_sample_weights = {}\n",
    "    y_sample = {}\n",
    "\n",
    "    # add indeces to dictionary and split them into XXX chunks\n",
    "    dt_idx = {}\n",
    "    dt_idx['train'] =  np.array_split(train_idx, config.chunk_split_size)\n",
    "    dt_idx['test'] =  np.array_split(test_idx, config.chunk_split_size)\n",
    "\n",
    "    for dt in ['train', 'test']:\n",
    "        print('-----')\n",
    "        print(dt)\n",
    "        print('-----')\n",
    "        for idx, part in enumerate(dt_idx[dt]):\n",
    "            if idx%20==0:\n",
    "                print('Partitions executed: ', idx, '/', len(dt_idx[dt]))\n",
    "            # define example (X)\n",
    "            sample_tokens = {}\n",
    "            for key in X_tokenized_subset.keys():\n",
    "                sample_tokens[key] = X_tokenized_subset[key][part, :]\n",
    "            #display('Sample tokens ', sample_tokens)\n",
    "\n",
    "            # retrieve ground truth\n",
    "            y_sample[dt] = y_masked[part, :]\n",
    "\n",
    "            # pull masked token indexes\n",
    "            sample_sample_weights[dt] = sample_weights[part, :]\n",
    "\n",
    "            # predict\n",
    "            mlm_sample_prediction[dt] = bert_masked_model.predict(sample_tokens)\n",
    "\n",
    "            # print shape\n",
    "            #print('Shape of mlm_tensor_predictions ', mlm_sample_prediction[dt].shape)\n",
    "\n",
    "            # compute accuracy\n",
    "            accuracy = utils_MLM_eval.all_patients_K_predictions_and_accuracy(\n",
    "            sample_tokens, y_sample[dt],\n",
    "            mlm_sample_prediction[dt], id2token['diag'],\n",
    "            mask_token_id\n",
    "            )\n",
    "\n",
    "            #print(dt + ' accuracy: %.3f'% (accuracy))\n",
    "            # append to global list\n",
    "            acc_master[dt].append(accuracy)\n",
    "\n",
    "            # compute APS and AUC\n",
    "            temp_metrics = {}\n",
    "            temp_metrics[dt] = utils_MLM_eval.all_patients_predictions_and_apc(\n",
    "                vect_layer,\n",
    "                sample_tokens, y_sample[dt],\n",
    "                mlm_sample_prediction[dt], id2token['diag'],\n",
    "                sample_sample_weights[dt],\n",
    "                #silence_print=True\n",
    "            )\n",
    "\n",
    "            # append to global list\n",
    "            aps_master[dt].append(temp_metrics[dt][temp_metrics[dt].metric.eq('APS')].value)\n",
    "            auc_master[dt].append(temp_metrics[dt][temp_metrics[dt].metric.eq('AUC')].value)\n",
    "            \n",
    "            \n",
    "    # print average metrics\n",
    "    print('----')\n",
    "    for dt in ['train', 'test']:\n",
    "        print(dt + ' ACC: %.3f'% (np.mean(acc_master[dt])))\n",
    "    print('----')\n",
    "    \n",
    "    for dt in ['train', 'test']:\n",
    "        print(dt + ' APS: %.3f'% (np.mean(aps_master[dt])))\n",
    "    print('----')\n",
    "    \n",
    "    for dt in ['train', 'test']:\n",
    "        print(dt + ' AUC: %.3f'% (np.mean(auc_master[dt])))\n",
    "    print('----')\n",
    "    \n",
    "    \n",
    "    # export amerage metrics to csv\n",
    "    metrics = pd.DataFrame()\n",
    "    for dt in ['train', 'test']:\n",
    "        temp= pd.DataFrame({\n",
    "            'metric':['ACC', 'APS', 'AUC'],\n",
    "            'value':[np.mean(acc_master[dt]), np.mean(aps_master[dt]),np.mean(auc_master[dt])],\n",
    "            'sample':[dt, dt, dt]\n",
    "            })\n",
    "        metrics = pd.concat([metrics, temp], axis=0)\n",
    "    \n",
    "    metrics.reset_index(drop=True, inplace=True)\n",
    "    metrics.to_csv('../embeddings/results/ApsAuc_' + model_name + '.csv')\n",
    "    \n",
    "# prin number of examples in train and test\n",
    "print('Train examples:', len(train_idx))\n",
    "print('Test examples:', len(test_idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
